{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"MLSS2020TU - Practical 1b.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SB0EeXzyu_sz"},"source":["# Practical 1b: Hyperparameter Tuning\n","\n","© Machine Learning Summer School - Telkom University\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oUkeuLWzoHoN"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%201/MLSS2020TU%20-%20Practical%201b.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","    \n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%201/MLSS2020TU%20-%20Practical%201b.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t7W-6mmQVO13"},"source":["---\n","## Hyperparameter\n","\n","Hyperparameters are variables whose value is set by the model designer before the learning process begins. These variables remain constant over the training process and directly impact the performance of your ML program.\n","\n","hyperparameters might address model design questions such as:\n","* model (architectural) hyperparameters:\n","  * degree of polynomial features in linear model\n","  * number of layers and neurons in Neural Network\n","  * type of layers to use in Deep Learning\n","  * etc.\n","\n","* training (algorithm) hyperparameters:\n","  * K value for Nearest Neighbors\n","  * learning rate for Gradient Descent\n","  * regularization\n","  * dropout value\n","  * etc.\n","\n","This is in contrast to the weights in Artificial Neural Network, which are the values of parameters that are learned through the algorithm. "]},{"cell_type":"markdown","metadata":{"id":"er_Y-N_wNXZn","colab_type":"text"},"source":["---\n","## Optimization Approaches\n","\n","There are several approaches to address this optimization problem. The most common of it are:\n","* Traditional Grid Search\n"," * commonly used to optimize width, depth, and types of layer in ANN\n"," * find the K value for KNN or Kernel type for SVMs\n","* Random Search\n"," * can be used to optimize the initial learning rate\n","* Bayesian opmitization\n"," * also used to optimize the initial learning rate\n","* Evolutionary optimization\n","\n","\n","<table>\n","  <tr><td  align=\"center\">\n","    <img src=\"https://images.ctfassets.net/be04ylp8y0qc/1m1AB8NPTcKkuEaqg2Zgyg/e56d45a2c85b99820ead8159a280323f/hp_tuning_flow_1f745fd5e0ae8830804bab6a66e2c917_1000.png\" width=\"80%\" \n","         alt=\"Hyperparameter tuning process\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure:</b>Hyperparameter tuning process.<br/>\n","  </td></tr>\n","</table>\n","\n","\n","Grid Search optimization usually incorporated with K-Fold Cross-Validation scheme, while the rest only uses Hold-out validation. Especially in learning with huge dataset, prefer one validation fold to cross-validation. In most cases a single validation set of respectable size substantially simplifies the code base, without the need for cross-validation with multiple folds\n","\n","For Neural Network and Deep Learning, Hyperparameter Tuning is usually only done with a little epoch in order to find a combination of hyperparameter that has the highest potential for the \"actual\" training process."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"flcLRgeYo-i-"},"source":["---\n","# [Part 0] Preparation"]},{"cell_type":"markdown","metadata":{"id":"SFYlyWBOT-wb","colab_type":"text"},"source":["## Import Modules"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GglshHl-WTgD","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","\n","%matplotlib inline\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2VMUU3lMcFMk"},"source":["## Load Fashion MNIST Dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BY-2CaubYh6L","colab":{}},"source":["(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJjIcXxIlv3x","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train[i+j*10], cmap='gray')\n","        ax[j,i].set_title(class_names[y_train[i+j*10]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HRVg3F5lwvu","colab_type":"code","colab":{}},"source":["X_val = X_train[-10000:,:]\n","y_val = y_train[-10000:]\n","\n","X_train = X_train[:-10000, :]\n","y_train = y_train[:-10000]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0JOlSYjl3tC","colab_type":"code","colab":{}},"source":["X_train = X_train.astype('float32') / 255.\n","X_val   = X_val.astype('float32') / 255.\n","X_test  = X_test.astype('float32') / 255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uv0Pngj-UCkp"},"source":["---\n","#[Part 1] Keras Tuner\n","\n","<img src = \"https://i.ibb.co/4RyZpZ2/keras-tuner.png\" height=\"200\" align = \"center\">\n","\n","Keras tuner is a hyperparameter optimization library (Hyperparameter tuner) built specifically for `tf.keras` with TensorFlow 2.0. It was introduced at the Google I/O 2019 event. \n","\n","This library is intended for AI practitioners, hypertuner algorithm creators, and model designers. It can be as simple as possible to build a model with a clean and easy API. Keras Tuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms.\n","\n","read more: [Documentation](https://keras-team.github.io/keras-tuner/), [github repository](https://github.com/keras-team/keras-tuner)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zsak3xpqWnay"},"source":["---\n","## 1 - Installation\n","\n","First, install Keras Tuner "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rBkyRMBxWp5-","colab":{}},"source":["!pip install -q -U keras-tuner"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HERQ6X7vpMjD","colab":{}},"source":["import kerastuner as kt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ZJ8uZqvU2_U","colab_type":"text"},"source":["---\n","## 2 - Define Hyperparameter Range\n","\n","In this example, we'll only use a Two-Layer Neural Network, and we're going to tune the layer width (number of hidden neuron), type of activation function, and the learning rate. Both hyperparameter range and the model need to be defined inside a method to feed into the tuner object. So in this part, we'll see the explanations first, then later the actual implementations."]},{"cell_type":"markdown","metadata":{"id":"lXGyRCQWYwG-","colab_type":"text"},"source":["### Layer Width\n","\n","For this hyperparameter, we'll use grid choice to iterate from 128 to 256 with 64 increment. For that we use `Int()` method as follow\n","\n","```python \n","    ch_unit = hp.Int('units', min_value=128, max_value=256, step=64) \n","```"]},{"cell_type":"markdown","metadata":{"id":"NflGUMLFYzzb","colab_type":"text"},"source":["### Activation Function\n","For the activation function, let's try `relu`, `tanh`, and `sigmoid` function. For that we use `Choice()` method as follow\n","```python\n","    ch_activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'], default='relu')\n","```"]},{"cell_type":"markdown","metadata":{"id":"1Vdgt3YpVc-u","colab_type":"text"},"source":["### Learning Rate\n","The learning rate represents how fast the learning algorithm progresses.The learning rate may, in fact, be the most important hyperparameter to configure for your model. Unfortunately, we cannot analytically calculate the optimal learning rate for a given model on a given dataset. Instead, a good (or good enough) learning rate must be discovered via trial and error. \n","\n","Arguably, it is actually not the best thing to tune learning rate using grid search. As mentioned by Bergstra and Bengio in [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf), “randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid”. \n","\n","<table>\n","  <tr><td  align=\"center\">\n","    <img src=\"https://cs231n.github.io/assets/nn3/gridsearchbad.jpeg\" width=\"50%\" \n","         alt=\"Grid vs Random Layout.\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure:</b> Grid vs Random Layout illustration from [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).\n","  </td></tr>\n","</table>\n","\n","Usually, the learning rate is chosen on a log scale. Intuitively, this is because learning rate have multiplicative effects on the training dynamics. This prior knowledge can be incorporated in the search through the setting of the sampling method:\n","\n","```python\n","    ch_learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='LOG', default=1e-3)\n","```"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uTc-kdRDcHnj"},"source":["## 3 - Define Model\n","\n","Now to define a function to build the model. We still use the usual Keras API, but feed in the choice for each hyperparameter. The function takes an argument hp from which we can use to sample the hyperparameters."]},{"cell_type":"markdown","metadata":{"id":"zig5dcxkvH66","colab_type":"text"},"source":["\n","> <font color='red'>**EXERCISE**: </font> Define your classification model. \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P-yA7c9BpGW3","colab":{}},"source":["def build_model(hp):\n","  \n","    ch_unit          = ??\n","    ch_activation    = ??\n","    ch_learning_rate = ??\n","\n","    # define model\n","    model = Sequential([\n","                Flatten(input_shape=(28,28)), \n","                Dense(units=??, activation=??),\n","                Dense(10, activation='softmax')\n","            ])\n","\n","    # compile model\n","    model.compile(\n","        optimizer = tf.keras.optimizers.Adam(??),\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy'])\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"le1WFCWccNBK"},"source":["---\n","# [Part 2] Initialize The Tuner\n","\n","The Keras Tuner has four tuners available - **RandomSearch**, **Hyperband**,  **Bayesian Optimization** and **Sklearn**. In this exercise, we'll focus on random search and Hyperband. We won't go into theory, but for Hyperband, its main idea is to optimize Random Search in terms of search time.\n","\n","In here we can specify the model-building function, the name of the `objective` (metrics) to optimize, the total number of trials (`max_trials`) to test, and the number of models that should be built and fit for each trial (`executions_per_trial`). \n","\n","Similar to K-Fold Cross Validation, the purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model.\n","\n"]},{"cell_type":"code","metadata":{"id":"eu8sjb-CcvF4","colab_type":"code","colab":{}},"source":["from kerastuner.tuners import RandomSearch, Hyperband"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3iQEt9tcw1i","colab_type":"text"},"source":["---\n","## 1 - Random Search Tuner\n","The most intuitive way to perform hyperparameter tuning is to randomly sample hyperparameter combinations and test them out. "]},{"cell_type":"markdown","metadata":{"id":"niOVEXeggO24","colab_type":"text"},"source":["### Initialize Random Tuner\n","Here we will do the tuning experiment 4 times, with the best model selection metrics based on validation accuracy. The `max_trials` variable represents the number of hyperparameter combinations that will be tested by the tuner, while the `execution_per_trial` variable is the number of models that should be built and fit for each trial for robustness purposes."]},{"cell_type":"code","metadata":{"id":"yJvke3Pgfj2Z","colab_type":"code","colab":{}},"source":["tuner_random = RandomSearch(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=4,\n","    executions_per_trial=2,\n","    directory='random_search',\n","    project_name='mnist_random'\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8FEYXuDfl5D","colab_type":"text"},"source":["Once the model and the tuner are set up, we can view the search space summary"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QabT6rs9Yh02","colab":{}},"source":["tuner_random.search_space_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GDm2MdxaeqJP"},"source":["### Run Random Tuning Process\n","\n","Now we can start the Optimization for just 5 epochs. The arguments for the search method are the same as those used for `tf.keras.model.fit` \n","\n","Depend on how big your search space is, it might take a while."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p3iaE6wNYhuz","colab":{}},"source":["tuner_random.search(X_train, y_train,\n","             batch_size=256,\n","             epochs=5,\n","             validation_data=(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NlC3tZh-ewyk"},"source":["### View the Search Summary\n","\n","Finally, the search results can be summarized and used"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aHopmbf2Y9V9","colab":{}},"source":["tuner_random.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"awBGWeVTe3fR"},"source":["### Show The Best Hyperparameter"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VAsR2KZWY-7Z","colab":{}},"source":["import tensorflow.keras.backend as K\n","\n","# Retrieve the best model.\n","best_model_random = tuner_random.get_best_models(num_models=1)[0]\n","best_model_random.summary()\n","\n","print('Best Initial Learning Rate =', K.eval(best_model_random.optimizer.lr))\n","print('Activation Function used   =', best_model_random.layers[1].get_config()['activation'],)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3VC8B97m_KK","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import plot_model\n","\n","plot_model(best_model_random, \n","           to_file='best_random.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZpKk4t-6pgVl"},"source":["---\n","## 2 - Hyperband Tuner\n","Hyperband is an optimized version of random search which uses early-stopping to speed up the hyperparameter tuning process. The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. \n","\n","\n","The main idea is to fit a large number of models for a small number of epochs and to only continue training for the models achieving the highest accuracy on the validation set. The `max_epochs` variable is the max number of epochs that a model can be trained for."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iZAHvmswpgVx"},"source":["### Initialize Hyperband Tuner\n","Here we will do the tuning experiment 4 times, with the best model selection metrics based on validation accuracy."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"By8J41vypgV2","colab":{}},"source":["tuner_hyperband = Hyperband(\n","    build_model,\n","    max_epochs=5,\n","    objective='val_accuracy',\n","    executions_per_trial=2,\n","    directory='hyperband',\n","    project_name='mnist_hyperband'\n",")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yoawe56cpgWC"},"source":["Once the model and the tuner are set up, we can view the search space summary"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y2obkTX_pgWE","colab":{}},"source":["tuner_hyperband.search_space_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pDqVbd0lpgWN"},"source":["### Run Hyperband Tuning Process\n","\n","Now we can start the Optimization for just 5 epochs. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tNftH8vIpgWO","colab":{}},"source":["tuner_hyperband.search(X_train, y_train,\n","             batch_size=256,\n","             epochs=5,\n","             validation_data=(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ur-FvOeupgWT"},"source":["### View the Search Summary\n","\n","Finally, the search results can be summarized and used"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oa8Mz9OBpgWU","colab":{}},"source":["tuner_hyperband.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"A0fKL9SFpgWZ"},"source":["### Show The Best Hyperparameter"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eUpeavJEpgWa","colab":{}},"source":["# Retrieve the best model.\n","best_model_hyperband = tuner_hyperband.get_best_models(num_models=1)[0]\n","best_model_hyperband.summary()\n","\n","print('Best Initial Learning Rate =', K.eval(best_model_hyperband.optimizer.lr))\n","print('Activation Function used   =', best_model_random.layers[1].get_config()['activation'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rwlVOzbLpgWe","colab":{}},"source":["from tensorflow.keras.utils import plot_model\n","\n","plot_model(best_model_hyperband, \n","           to_file='best_hyperband.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HqHuRjb7fEY1"},"source":["---\n","## 3 - Evaluate Best Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ghfckNyzWTdd","colab":{}},"source":["# loss, accuracy = best_model_random.evaluate(X_test, y_test)\n","\n","loss, accuracy = best_model_hyperband.evaluate(X_test, y_test)\n","\n","print('Validation Accuracy =',accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vnCN6_Fp6E57"},"source":["---\n","### View Result\n","Now to visualize some of the model's predictions:"]},{"cell_type":"code","metadata":{"id":"6nFBOMSPkwxa","colab_type":"code","colab":{}},"source":["# y_pred = best_model_random.predict(X_test, verbose=0)\n","\n","y_pred = best_model_hyperband.predict(X_test, verbose=0)\n","y_pred = np.argmax(y_pred, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZyipqbgktTg","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(2,8,figsize=(22,6))\n","fig.subplots_adjust(hspace=0.2, wspace=0.4)\n","\n","for j in range(0,2):\n","    for i in range(0, 8):\n","\n","        img_index = np.random.randint(0, 10000)\n","        ax[j,i].imshow(X_test[img_index], cmap='gray')\n","\n","        actual_label    = int(y_test[img_index])\n","        predicted_label = int(y_pred[img_index])\n","\n","        color = 'red'\n","        if actual_label == predicted_label:\n","            color = 'green'\n","\n","        ax[j,i].set_title(\"Actual: {} ({})\\n Pred: {} ({})\".format(\n","            actual_label, class_names[actual_label], predicted_label, class_names[predicted_label]\n","            ), color=color)\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quMA17JKDnpI"},"source":["---\n","\n","# Congratulation\n","\n","<font size=5> You've Completed Practical 1b</font>\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]}]}